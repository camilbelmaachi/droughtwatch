{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EfficientNetB3 Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "import argparse\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import layers, initializers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from keras.models import model_from_json\n",
    "from tensorflow.keras.applications import EfficientNetB3\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Resizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUM_TRAIN = 16000\n",
    "# NUM_VAL = 3200\n",
    "IMG_DIM = 65\n",
    "NUM_CLASSES = 4\n",
    "TOTAL_TRAIN = 100\n",
    "TOTAL_VAL = 50\n",
    "# TOTAL_TRAIN2 = 86317\n",
    "# TOTAL_VAL2 = 10778"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_list_from_folder(folder, data_path):\n",
    "    folderpath = os.path.join(data_path, folder)\n",
    "    filelist = []\n",
    "    for filename in os.listdir(folderpath):\n",
    "        if filename.startswith('part-') and not filename.endswith('gstmp'):\n",
    "            filelist.append(os.path.join(folderpath, filename))\n",
    "    return filelist\n",
    "\n",
    "def load_data(data_path):\n",
    "    train = file_list_from_folder(\"train\", data_path)\n",
    "    val = file_list_from_folder(\"val\", data_path)\n",
    "    return train, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {\n",
    "  'B1': tf.io.FixedLenFeature([], tf.string),\n",
    "  'B2': tf.io.FixedLenFeature([], tf.string),\n",
    "  'B3': tf.io.FixedLenFeature([], tf.string),\n",
    "  'B4': tf.io.FixedLenFeature([], tf.string),\n",
    "  'B5': tf.io.FixedLenFeature([], tf.string),\n",
    "  'B6': tf.io.FixedLenFeature([], tf.string),\n",
    "  'B7': tf.io.FixedLenFeature([], tf.string),\n",
    "  'B8': tf.io.FixedLenFeature([], tf.string),\n",
    "  'B9': tf.io.FixedLenFeature([], tf.string),\n",
    "  'B10': tf.io.FixedLenFeature([], tf.string),\n",
    "  'B11': tf.io.FixedLenFeature([], tf.string),\n",
    "  'label': tf.io.FixedLenFeature([], tf.int64),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tfrecords(filelist, batch_size, buffer_size, include_viz=False):\n",
    "    # try a subset of possible bands\n",
    "    def _parse_(serialized_example, keylist=['B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8']):\n",
    "        example = tf.io.parse_single_example(serialized_example, features)\n",
    "\n",
    "        def getband(example_key):\n",
    "            img = tf.io.decode_raw(example_key, tf.uint8)\n",
    "            return tf.reshape(img[:IMG_DIM**2], shape=(IMG_DIM, IMG_DIM, 1))\n",
    "\n",
    "        bandlist = [getband(example[key]) for key in keylist]\n",
    "        # combine bands into tensor\n",
    "        image = tf.concat(bandlist, -1)\n",
    "\n",
    "        # one-hot encode ground truth labels\n",
    "        label = tf.cast(example['label'], tf.int32)\n",
    "        label = tf.one_hot(label, NUM_CLASSES)\n",
    "\n",
    "        return {'image': image}, label\n",
    "\n",
    "    tfrecord_dataset = tf.data.TFRecordDataset(filelist)\n",
    "    tfrecord_dataset = tfrecord_dataset.map(lambda x:_parse_(x)).shuffle(buffer_size).repeat(-1).batch(batch_size)\n",
    "    tfrecord_iterator = tfrecord_dataset.make_one_shot_iterator()\n",
    "    image, label = tfrecord_iterator.get_next()\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(hist):\n",
    "    plt.plot(hist.history[\"accuracy\"])\n",
    "    plt.plot(hist.history[\"val_accuracy\"])\n",
    "    plt.title(\"model accuracy\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_preprocess(image, label):\n",
    "    label = tf.one_hot(label, NUM_CLASSES)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n",
      "WARNING:tensorflow:From <ipython-input-7-ca5c2c68f2f5>:22: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This is a deprecated API that should only be used in TF 1 graph mode and legacy TF 2 graph mode available through `tf.compat.v1`. In all other situations -- namely, eager mode and inside `tf.function` -- you can consume dataset elements using `for elem in dataset: ...` or by explicitly creating iterator via `iterator = iter(dataset)` and fetching its elements via `values = next(iterator)`. Furthermore, this API is not available in TF 2. During the transition from TF 1 to TF 2 you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)` to create a TF 1 graph mode style iterator for a dataset created through TF 2 APIs. Note that this should be a transient state of your code base as there are in general no guarantees about the interoperability of TF 1 and TF 2 code.\n",
      "Finished loading data!\n",
      "Preparing hold out...\n",
      "deleting unused variables...\n"
     ]
    }
   ],
   "source": [
    "print(\"loading data\")\n",
    "train_tfrecords, val_tfrecords = load_data(\"../droughtwatch/data/\")\n",
    "train_images, train_labels = parse_tfrecords(train_tfrecords, TOTAL_TRAIN, TOTAL_TRAIN)\n",
    "val_images, val_labels = parse_tfrecords(val_tfrecords, TOTAL_VAL, TOTAL_VAL)\n",
    "print(\"Finished loading data!\")\n",
    "print(\"Preparing hold out...\")\n",
    "# #Divide the data in a train set, a validation set, and a test set and store it in variables as tensors\n",
    "k = int((2/3)*TOTAL_TRAIN)\n",
    "X_tr = train_images[\"image\"][:k]\n",
    "y_tr = train_labels[:k]\n",
    "X_val = train_images[\"image\"][k:]\n",
    "y_val = train_labels[k:]\n",
    "X_test = val_images[\"image\"]\n",
    "y_test = val_labels\n",
    "\n",
    "print(\"deleting unused variables...\")\n",
    "del train_tfrecords\n",
    "del val_tfrecords\n",
    "del val_images\n",
    "del val_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering blank and black images\n",
      "finished filtering blank and black images\n",
      "converting images to rgb ...\n",
      "finished converting images to rgb\n",
      "deleting unused variables\n"
     ]
    }
   ],
   "source": [
    "# #Keep only images that are not all blank nor all black and convert the tensors as np arrays\n",
    "print(\"Filtering blank and black images\")\n",
    "indices = np.where([i[i.std() >= 10].all() for i in X_tr.numpy()])\n",
    "X_tr, y_tr = X_tr.numpy(), y_tr.numpy()\n",
    "X_tr, y_tr = X_tr[indices], y_tr[indices]\n",
    "\n",
    "indices = np.where([i[i.std() >= 10].all() for i in X_val.numpy()])\n",
    "X_val, y_val = X_val.numpy(), y_val.numpy()\n",
    "X_val, y_val = X_val[indices], y_val[indices]\n",
    "\n",
    "indices = np.where([i[i.std() >= 10].all() for i in X_test.numpy()])\n",
    "X_test, y_test = X_test.numpy(), y_test.numpy()\n",
    "X_test, y_test = X_test[indices], y_test[indices]\n",
    "print(\"finished filtering blank and black images\")\n",
    "# #Keep only rgb channels for the vgg16 model\n",
    "print(\"converting images to rgb ...\")\n",
    "X_trrgb = X_tr[:,:,:,1:4]\n",
    "X_valrgb = X_val[:,:,:,1:4]\n",
    "X_testrgb = X_test[:,:,:,1:4]\n",
    "print(\"finished converting images to rgb\")\n",
    "\n",
    "print(\"deleting unused variables\")\n",
    "del X_tr\n",
    "del X_val\n",
    "del X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb3_notop.h5\n",
      "43941888/43941136 [==============================] - 16s 0us/step\n",
      "Epoch 1/1000\n",
      "3/3 [==============================] - 57s 19s/step - loss: 12.3347 - accuracy: 0.3182 - val_loss: 12.3403 - val_accuracy: 0.7353\n",
      "Epoch 2/1000\n",
      "3/3 [==============================] - 28s 9s/step - loss: 4.5166 - accuracy: 0.7273 - val_loss: 9.1906 - val_accuracy: 0.7353\n",
      "Epoch 3/1000\n",
      "3/3 [==============================] - 27s 9s/step - loss: 2.9400 - accuracy: 0.6970 - val_loss: 4.3218 - val_accuracy: 0.7353\n",
      "Epoch 4/1000\n",
      "3/3 [==============================] - 30s 10s/step - loss: 1.6632 - accuracy: 0.5606 - val_loss: 0.9857 - val_accuracy: 0.7353\n",
      "Epoch 5/1000\n",
      "3/3 [==============================] - 32s 11s/step - loss: 1.1737 - accuracy: 0.4394 - val_loss: 0.8305 - val_accuracy: 0.7353\n",
      "Epoch 6/1000\n",
      "3/3 [==============================] - 29s 10s/step - loss: 0.8571 - accuracy: 0.6667 - val_loss: 0.9551 - val_accuracy: 0.7353\n",
      "Epoch 7/1000\n",
      "3/3 [==============================] - 27s 9s/step - loss: 1.1739 - accuracy: 0.7121 - val_loss: 1.4200 - val_accuracy: 0.0882\n",
      "Epoch 8/1000\n",
      "3/3 [==============================] - 27s 9s/step - loss: 0.6376 - accuracy: 0.7273 - val_loss: 1.5517 - val_accuracy: 0.1176\n",
      "Epoch 9/1000\n",
      "3/3 [==============================] - 52s 17s/step - loss: 0.5433 - accuracy: 0.7576 - val_loss: 1.1560 - val_accuracy: 0.1471\n",
      "Epoch 10/1000\n",
      "3/3 [==============================] - 34s 11s/step - loss: 0.6473 - accuracy: 0.7576 - val_loss: 1.0649 - val_accuracy: 0.5588\n",
      "Epoch 11/1000\n",
      "3/3 [==============================] - 30s 10s/step - loss: 0.8133 - accuracy: 0.7727 - val_loss: 1.7561 - val_accuracy: 0.1176\n",
      "Epoch 12/1000\n",
      "3/3 [==============================] - 32s 11s/step - loss: 0.5245 - accuracy: 0.7727 - val_loss: 1.7038 - val_accuracy: 0.0882\n",
      "Epoch 13/1000\n",
      "3/3 [==============================] - 28s 9s/step - loss: 0.4854 - accuracy: 0.7879 - val_loss: 2.0926 - val_accuracy: 0.2059\n",
      "Epoch 14/1000\n",
      "3/3 [==============================] - 50s 17s/step - loss: 0.5946 - accuracy: 0.8485 - val_loss: 4.1969 - val_accuracy: 0.0882\n",
      "Epoch 15/1000\n",
      "3/3 [==============================] - 27s 9s/step - loss: 0.7728 - accuracy: 0.8333 - val_loss: 3.8720 - val_accuracy: 0.0882\n",
      "Epoch 16/1000\n",
      "3/3 [==============================] - 26s 9s/step - loss: 0.9917 - accuracy: 0.7727 - val_loss: 1.7578 - val_accuracy: 0.7059\n",
      "Epoch 17/1000\n",
      "3/3 [==============================] - 28s 9s/step - loss: 0.6326 - accuracy: 0.7727 - val_loss: 2.6322 - val_accuracy: 0.7353\n",
      "Epoch 18/1000\n",
      "3/3 [==============================] - 28s 9s/step - loss: 1.3125 - accuracy: 0.7727 - val_loss: 2.9239 - val_accuracy: 0.7353\n",
      "Epoch 19/1000\n",
      "3/3 [==============================] - 30s 10s/step - loss: 0.7690 - accuracy: 0.7879 - val_loss: 2.6930 - val_accuracy: 0.7353\n",
      "Epoch 20/1000\n",
      "3/3 [==============================] - 29s 10s/step - loss: 0.6556 - accuracy: 0.7576 - val_loss: 2.3440 - val_accuracy: 0.7353\n",
      "Epoch 21/1000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7230 - accuracy: 0.6515 Restoring model weights from the end of the best epoch.\n",
      "3/3 [==============================] - 29s 10s/step - loss: 0.7230 - accuracy: 0.6515 - val_loss: 1.9971 - val_accuracy: 0.7353\n",
      "Epoch 00021: early stopping\n"
     ]
    }
   ],
   "source": [
    "#Build EfficientnetB3 model\n",
    "IMG_SIZE = 300\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator()\n",
    "datagen2 = tf.keras.preprocessing.image.ImageDataGenerator()\n",
    "# compute quantities required for featurewise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied)\n",
    "datagen.fit(X_trrgb)\n",
    "datagen2.fit(X_valrgb)\n",
    "\n",
    "# ----------------\n",
    "inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "x = inputs\n",
    "x = Resizing(300, 300)(x)\n",
    "activationnetB3 = EfficientNetB3(include_top=False, weights = \"imagenet\")(x)\n",
    "outputsflatten = layers.Flatten()(activationnetB3)\n",
    "outputsdense1 = layers.Dense(64, activation = \"relu\")(outputsflatten)\n",
    "outputsdense2 = layers.Dense(4, activation = \"softmax\")(outputsdense1)\n",
    "model = tf.keras.Model(inputs, outputsdense2)\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "es = EarlyStopping(monitor='val_accuracy', mode='max', patience=20, verbose=1, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(datagen.flow(X_trrgb, y_tr, batch_size=32),\\\n",
    "         epochs=1000, validation_data = datagen2.flow(X_valrgb, y_val, batch_size = 32), verbose = 1,\\\n",
    "                    callbacks=[es])\n",
    "del X_trrgb\n",
    "del X_valrgb\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
